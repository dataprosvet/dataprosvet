---
title: сверточные сети
tags:
  - cnn
aliases:
  - cnn
  - сверточные_нейросети
  - свертки
  - convolution
pulishing_date: 2023-08-10 18:00
category: Топик
---
## Определение

**Свертки (2D convolutions)** - представляют собой основной строительный блок сверточных нейронных сетей и нашли широкое применение в обработке различных видов данных, включая аудио, текст и даже временные ряды. Однако их популярность достигла пика при применении к задачам классификации изображений.

Изображения можно рассматривать как матрицы пикселей. Для представления черно-белых изображений используется один канал (Grayscale), в то время как для цветных изображений применяются три канала (RGB - красный, зеленый, синий).

![[Files/image_rgb.png]]


Как обрабатывали изображения до появления сверток? Это было довольно просто - все пиксели изображения последовательно собирали в один массив. Можете представить, насколько большим может быть такой массив для hq изображения. Это первое ограничение такого подхода. Второе ограничение заключается в том, что при переходе к такому представлению мы теряем информацию о пространственном расположении объектов на изображении.

Чтобы объяснить свертки, давайте проведем аналогию с применением фильтра к изображению. Допустим, у нас есть фильтр размером 3x3, и мы хотим применить его к черно-белому изображению с одним каналом.


![[Files/feature_map_creation.png]]

Свертки (2D convolutions) выполняют поэлементное умножение между двумя массивами, `region` и `filter`, а затем суммируют все полученные значения. Эта операция по сути является скалярным произведением (dot product).

![[Files/feature_map_dot_product.png]]

Свертки позволяют нам применить фильтр к изображению и получить интересующие нас данные, сохраняя при этом пространственную информацию.

Для регулирования размера выходного изображения и выбора данных, которые будут взяты в окне свертки, используют параметры `padding` и `striding`.

- **Padding** расширяет входную матрицу путем добавления нужного количества дополнительных строк и столбцов, чаще всего заполняемых нулями.
- **Striding** позволяет задать шаг, с которым будет применяться свертка к изображению. Этот параметр влияет на размер выходного изображения.

После применения свертки к изображению, обычно используют активационную функцию, такую как ReLU, для выделения "feature maps" и далее применяют операцию пулинга (pooling) для уменьшения размера "feature map", при этом сохраняя важные данные.


![[Files/feature_maps_polling.png]]

**Max Pooling**: в этой операции выбирается максимальное значение из окна, и оно сохраняется. Максимальное пулинг часто используется для выделения наиболее активных функций в данном окне

**Average Pooling**: здесь вычисляется среднее значение всех элементов в окне, может помочь сгладить данные и уменьшить размерность, сохраняя при этом основную информацию.

**Sum Pooling**: суммируются все значения в окне. Это может быть полезно, если важна суммарная информация.

![[Files/cnn_architectiure.png]]

В данном примере представлена простейшая архитектура сверточной нейронной сети (CNN). Современные нейронные сети имеют гораздо более сложные архитектуры с множеством слоев и разнообразными подходами. Например, архитектуры, объединяющие принципы трансформера и сверточных сетей, такие как Conformer, получили широкое распространение.

Классические модели CNN, такие как ResNet, EfficientNet, MobileNet и другие, используются в качестве "основы" (backbone) в более сложных архитектурах. Они внесли значительный вклад в развитие нейронных сетей и остаются важными инструментами и по сей день.

![[Files/MNIST.png]]

Давайте рассмотрим известный набор данных MNIST, который является игрушечным датасетом, содержащим изображения рукописных цифр от 0 до 9. Каждое изображение в этом датасете имеет размер 28 × 28 пикселей. Для решения задачи с использованием фреймворка PyTorch, нам необходимо создать простейшую нейронную сеть, нам нужно определить все необходимые слои: сверточные (convolutional), активации (activation), пуллинг (pooling) и полносвязанные (fully connected) слои. Также мы должны написать код для предварительной обработки данных и цикл обучения нейронной сети.

## Пример использования

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import StepLR
import torchvision
import torchvision.transforms as transforms

# Define your neural network
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)
        self.relu = nn.ReLU()
        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)
        self.fc = nn.Linear(16 * 14 * 14, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = self.relu(x)
        x = self.maxpool(x)
        x = x.view(x.size(0), -1)  # Flatten the tensor
        x = self.fc(x)
        return x

transform=transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
        ])
dataset1 = datasets.MNIST('../data', train=True, download=True,
                   transform=transform)
dataset2 = datasets.MNIST('../data', train=False,
                   transform=transform)
train_loader = torch.utils.data.DataLoader(dataset1, batch_size=10, shuffle=True)
test_loader = torch.utils.data.DataLoader(dataset2, batch_size=10, shuffle=False)


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = Net().to(device)

# определяем основные компоненты обучения loss function, optimezer, sheduler
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adadelta(model.parameters(), lr=1.0)
scheduler = StepLR(optimizer, step_size=1, gamma=0.7)

# тренировка модели 10 эпох
for epoch in range(1, 11):
    model.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)  # Use CrossEntropyLoss here
        loss.backward()
        optimizer.step()
        if batch_idx % 100 == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * len(data), len(train_loader.dataset),
                100. * batch_idx / len(train_loader), loss.item()))

```


## Связи

## Ссылки

https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks
https://github.com/sooftware/conformer



